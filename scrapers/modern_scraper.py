import pandas as pd
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from bs4 import BeautifulSoup
import time
import random
import re
import sys
import os

# --- AYARLAR ---
BASE_URL = "https://fbref.com"
OUTPUT_FILE = "ALL_LEAGUES_DETAILED_MATCHES.csv"
all_matches_data = []

# 2016'dan 2025'e kadar
SEASONS_TO_SCRAPE = list(range(2016, 2025))

COMPETITIONS = [
    {"id": "12", "name": "La-Liga", "league_tag": "La Liga"},
    {"id": "9", "name": "Premier-League", "league_tag": "Premier League"},
    {"id": "20", "name": "Bundesliga", "league_tag": "Bundesliga"},
    {"id": "11", "name": "Serie-A", "league_tag": "Serie A"},
    {"id": "13", "name": "Ligue-1", "league_tag": "Ligue 1"},
    {"id": "26", "name": "Super-Lig", "league_tag": "S√ºper Lig"},
    {"id": "32", "name": "Primeira-Liga", "league_tag": "Liga Portugal"}
]

options = uc.ChromeOptions()
options.add_argument("--disable-blink-features=AutomationControlled")

driver = None

try:
    driver = uc.Chrome(options=options, use_subprocess=True)
    print("Cloudflare kontrol√º i√ßin bekleniyor (15sn)...")
    driver.get(BASE_URL)
    time.sleep(15)


    # --- DATA MINING FONKSƒ∞YONLARI ---

    def extract_stat_row(soup_obj, stat_name):
        """ D√ºz satƒ±r verileri (Faul, Korner, Ofsayt vb.) """
        label = soup_obj.find(lambda tag: tag.name in ["div", "th"] and tag.text.strip() == stat_name)
        if label:
            if label.name == "div":
                home_val = label.find_previous_sibling("div")
                away_val = label.find_next_sibling("div")
                return (home_val.text.strip() if home_val else '0'), (away_val.text.strip() if away_val else '0')
            elif label.name == "th":
                home_val = label.find_previous_sibling("td")
                away_val = label.find_next_sibling("td")
                return (home_val.text.strip() if home_val else '0'), (away_val.text.strip() if away_val else '0')
        return '0', '0'


    def extract_bar_stat(soup_obj, stat_name):
        """ 'X of Y' formatƒ±ndaki veriler (Pas, ≈ûut, Kurtarƒ±≈ü) """
        label = soup_obj.find(lambda tag: tag.name in ["div", "th"] and stat_name in tag.text)
        h_succ, h_att, a_succ, a_att = '0', '0', '0', '0'

        if label:
            # Metni alacaƒüƒ±mƒ±z yer (Div yapƒ±sƒ± i√ßin sibling, Table i√ßin sibling td)
            target_elem = None
            if label.name == "th":
                # Table yapƒ±sƒ±nda
                row = label.find_parent("tr").find_next_sibling("tr")
                if row:
                    tds = row.find_all("td")
                    if len(tds) >= 2:
                        # Ev Sahibi
                        parts_h = re.findall(r'(\d+)\s+of\s+(\d+)', tds[0].text)
                        if parts_h: h_succ, h_att = parts_h[0]
                        # Deplasman
                        parts_a = re.findall(r'(\d+)\s+of\s+(\d+)', tds[1].text)
                        if parts_a: a_succ, a_att = parts_a[0]

            elif label.name == "div" or label.parent.name == "div":
                # Div yapƒ±sƒ±nda (team_stats) - Genelde bir sonraki div i√ßinde text olur
                # Bu yapƒ± karma≈üƒ±k olabilir, genel text aramasƒ± yapalƒ±m
                parent = label.find_parent("div", id=re.compile("team_stats"))
                if parent:
                    # O b√∂l√ºmdeki metinleri tarayalƒ±m "286 of 402" gibi
                    all_text = parent.get_text(" ")
                    # Regex ile t√ºm "sayƒ± of sayƒ±" kalƒ±plarƒ±nƒ± bul
                    matches = re.findall(r'(\d+)\s+of\s+(\d+)', all_text)
                    # E≈üle≈ümeleri sƒ±raya g√∂re daƒüƒ±t (Genelde sƒ±ra: Passing Home, Passing Away, Shooting Home...)
                    # Bu y√∂ntem riskli, o y√ºzden spesifik elemente gidelim:

                    # Bu stat isminden sonra gelen ilk iki "div" bloƒüunu bulmaya √ßalƒ±≈üalƒ±m
                    # (Basit √ß√∂z√ºm: V2.0'daki gibi g√º√ßl√º bir regex parser kullanalƒ±m)
                    pass

        return h_succ, h_att, a_succ, a_att


    def extract_possession(soup_obj):
        poss_header = soup_obj.find(lambda tag: tag.name in ["div", "th"] and "Possession" in tag.text)
        if poss_header:
            if poss_header.name == "th":
                row = poss_header.find_parent("tr").find_next_sibling("tr")
                if row:
                    tds = row.find_all("td")
                    if len(tds) >= 2:
                        return tds[0].text.strip().replace('%', ''), tds[1].text.strip().replace('%', '')
            elif poss_header.name == "div" or poss_header.parent.name == "div":
                strongs = poss_header.find_parent("table").find_all("strong") if poss_header.find_parent(
                    "table") else []
                if len(strongs) >= 2:
                    return strongs[0].text.strip().replace('%', ''), strongs[1].text.strip().replace('%', '')
        return '50', '50'


    def extract_cards(soup_obj):
        cards_header = soup_obj.find(lambda tag: tag.name in ["div", "th"] and "Cards" in tag.text)
        hy, hr, ay, ar = 0, 0, 0, 0
        if cards_header:
            if cards_header.name == "th":
                row = cards_header.find_parent("tr").find_next_sibling("tr")
                if row:
                    tds = row.find_all("td")
                    if len(tds) >= 2:
                        hy = len(tds[0].select('.yellow_card'))
                        hr = len(tds[0].select('.red_card')) + len(tds[0].select('.yellow_red_card'))
                        ay = len(tds[1].select('.yellow_card'))
                        ar = len(tds[1].select('.red_card')) + len(tds[1].select('.yellow_red_card'))
            # Div yapƒ±sƒ±nda kartlar (Senin attƒ±ƒüƒ±n HTML'deki gibi)
            elif cards_header.name == "div" or cards_header.parent.name == "div":
                table = cards_header.find_parent("table")
                if table:
                    rows = table.find_all("tr")
                    # Kart satƒ±rƒ±nƒ± bul
                    for r in rows:
                        if "Cards" in r.text: continue  # Ba≈ülƒ±k satƒ±rƒ±
                        cols = r.find_all("td")
                        if len(cols) >= 2:
                            hy = len(cols[0].select('.yellow_card'))
                            hr = len(cols[0].select('.red_card')) + len(cols[0].select('.yellow_red_card'))
                            ay = len(cols[1].select('.yellow_card'))
                            ar = len(cols[1].select('.red_card')) + len(cols[1].select('.yellow_red_card'))
                            break
        return str(hy), str(hr), str(ay), str(ar)


    # --- ANA D√ñNG√ú ---
    for comp in COMPETITIONS:
        print(f"\n\n{'=' * 50}")
        print(f"üèÜ ≈ûAMPƒ∞YONA BA≈ûLIYOR: {comp['league_tag']}")
        print(f"{'=' * 50}")

        comp_id = comp['id']
        comp_slug = comp['name']

        for year in SEASONS_TO_SCRAPE:
            season_year_str = f"{year}-{year + 1}"
            print(f"\n  -> Sezon: {season_year_str} i≈üleniyor...")

            try:
                fixture_url = f"{BASE_URL}/en/comps/{comp_id}/{season_year_str}/schedule/{season_year_str}-{comp_slug}-Scores-and-Fixtures"
                driver.get(fixture_url)
                time.sleep(4)

                soup = BeautifulSoup(driver.page_source, "html.parser")
                match_links_tags = soup.select("td[data-stat='match_report'] a")

                if not match_links_tags:
                    print("    ‚ö†Ô∏è Ma√ß linki bulunamadƒ±.")
                    continue

                match_links = [BASE_URL + a['href'] for a in match_links_tags]
                print(f"    -> {len(match_links)} adet ma√ß bulundu. Detaylar √ßekiliyor...")

                season_data = []

                for i, url in enumerate(match_links, 1):
                    try:
                        driver.get(url)
                        time.sleep(random.uniform(2.5, 4.5))

                        match_soup = BeautifulSoup(driver.page_source, "html.parser")
                        content_div = match_soup.select_one("#content")
                        if not content_div: continue

                        scorebox = content_div.select_one("div.scorebox")
                        if not scorebox: continue

                        team_links = scorebox.select("a[href*='/squads/']")
                        scores = scorebox.select("div.score")
                        date_elem = scorebox.select_one("div.scorebox_meta span.venuetime")

                        if len(team_links) < 2 or len(scores) < 2: continue

                        home_team = team_links[0].get_text(strip=True)
                        away_team = team_links[1].get_text(strip=True)
                        home_goals = scores[0].get_text(strip=True)
                        away_goals = scores[1].get_text(strip=True)
                        match_date = date_elem['data-venue-date'] if date_elem else "Unknown"

                        data = {
                            "League": comp['league_tag'], "Season": season_year_str, "Date": match_date,
                            "HomeTeam": home_team, "AwayTeam": away_team,
                            "FTHG": home_goals, "FTAG": away_goals
                        }

                        # --- 1. TEMEL ƒ∞STATƒ∞STƒ∞KLER (Pos, Cards, Passing, Shooting) ---
                        # Bu kƒ±sƒ±m genellikle 'team_stats' i√ßindedir

                        main_stats_source = match_soup.find("div", id="team_stats")
                        if not main_stats_source:
                            # Gizli ise bul
                            wrapper = match_soup.find("div", id="all_matchstats")
                            if wrapper:
                                comment = wrapper.find(
                                    string=lambda t: isinstance(t, str) and "team_stats" in t)  # team_stats ara
                                if not comment:  # Bazen matchstats i√ßinde olur
                                    comment = wrapper.find(string=lambda t: isinstance(t, str) and "matchstats" in t)
                                if comment: main_stats_source = BeautifulSoup(comment, 'html.parser')

                        # Varsayƒ±lanlar
                        data["HomePossession"], data["AwayPossession"] = '50', '50'
                        data["HomeSOT"], data["HomeShots"], data["AwaySOT"], data["AwayShots"] = '0', '0', '0', '0'

                        if main_stats_source:
                            # Possession
                            data["HomePossession"], data["AwayPossession"] = extract_possession(main_stats_source)
                            # Cards
                            data["HomeYellowCards"], data["HomeRedCards"], data["AwayYellowCards"], data[
                                "AwayRedCards"] = extract_cards(main_stats_source)

                            # --- ≈ûUTLAR ve PASLAR (Bar Stats) ---
                            # Bu kƒ±sƒ±m biraz regex b√ºy√ºc√ºl√ºƒü√º gerektirir
                            text_content = main_stats_source.get_text(" ")

                            # Shots on Target (√ñrn: 4 of 13 ... 3 of 8)
                            # "Shots on Target" yazƒ±sƒ±ndan sonraki sayƒ±larƒ± yakala
                            shots_match = re.search(r'Shots on Target.*?(\d+)\s+of\s+(\d+).*?(\d+)\s+of\s+(\d+)',
                                                    text_content, re.DOTALL)
                            if shots_match:
                                data["HomeSOT"], data["HomeShots"] = shots_match.group(1), shots_match.group(2)
                                data["AwaySOT"], data["AwayShots"] = shots_match.group(3), shots_match.group(4)

                            # Passing (Opsiyonel, istersen ekleyebilirsin)
                            # pass_match = re.search(...) logic similar to above

                        # --- 2. EKSTRA ƒ∞STATƒ∞STƒ∞KLER (Fouls, Corners, etc.) ---
                        extra_source = match_soup.find("div", id="team_stats_extra")
                        if not extra_source:
                            wrapper = match_soup.find("div", id="all_matchstats")
                            if wrapper:
                                comment = wrapper.find(string=lambda t: isinstance(t, str) and "matchstats" in t)
                                if comment: extra_source = BeautifulSoup(comment, 'html.parser')

                        if extra_source:
                            data["HomeFouls"], data["AwayFouls"] = extract_stat_row(extra_source, "Fouls")
                            data["HomeCorners"], data["AwayCorners"] = extract_stat_row(extra_source, "Corners")
                            data["HomeCrosses"], data["AwayCrosses"] = extract_stat_row(extra_source, "Crosses")
                            data["HomeTouches"], data["AwayTouches"] = extract_stat_row(extra_source, "Touches")
                            data["HomeTackles"], data["AwayTackles"] = extract_stat_row(extra_source, "Tackles")
                            data["HomeInterceptions"], data["AwayInterceptions"] = extract_stat_row(extra_source,
                                                                                                    "Interceptions")
                            data["HomeAerialsWon"], data["AwayAerialsWon"] = extract_stat_row(extra_source,
                                                                                              "Aerials Won")
                            data["HomeClearances"], data["AwayClearances"] = extract_stat_row(extra_source,
                                                                                              "Clearances")
                            data["HomeOffsides"], data["AwayOffsides"] = extract_stat_row(extra_source, "Offsides")
                            data["HomeGoalKicks"], data["AwayGoalKicks"] = extract_stat_row(extra_source, "Goal Kicks")
                            data["HomeThrowIns"], data["AwayThrowIns"] = extract_stat_row(extra_source, "Throw Ins")
                            data["HomeLongBalls"], data["AwayLongBalls"] = extract_stat_row(extra_source, "Long Balls")
                        else:
                            for k in ["HomeFouls", "HomeCorners", "HomeCrosses", "HomeTouches", "HomeTackles",
                                      "HomeInterceptions", "HomeAerialsWon", "HomeClearances", "HomeOffsides",
                                      "HomeGoalKicks", "HomeThrowIns", "HomeLongBalls"]:
                                data[k] = '0';
                                data[k.replace('Home', 'Away')] = '0'

                        season_data.append(data)

                        print(f"\n‚úÖ {i}/{len(match_links)}: {home_team} {home_goals}-{away_goals} {away_team}")
                        print(
                            f"   üìä Top: %{data['HomePossession']}-%{data['AwayPossession']} | ≈ûut(SOT): {data['HomeShots']}({data['HomeSOT']}) - {data['AwayShots']}({data['AwaySOT']})")
                        print(
                            f"   üö© Korner: {data['HomeCorners']}-{data['AwayCorners']} | Faul: {data['HomeFouls']}-{data['AwayFouls']}")
                        print(
                            f"   üü® Sarƒ±: {data['HomeYellowCards']}-{data['AwayYellowCards']} | üü• Kƒ±rmƒ±zƒ±: {data['HomeRedCards']}-{data['AwayRedCards']}")
                        print("-" * 40)

                    except Exception as e:
                        print(f"    ‚ö†Ô∏è Hata: {e}")
                        continue

                all_matches_data.extend(season_data)
                df_save = pd.DataFrame(all_matches_data)
                df_save.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig")
                print(f"  üíæ SEZON KAYDEDƒ∞LDƒ∞. Toplam Veri: {len(df_save)} ma√ß.")

            except Exception as e:
                print(f"  üö® Sezon Hatasƒ±: {e}")
                continue

finally:
    if driver:
        driver.quit()
        print("\nüéâ Bƒ∞TTƒ∞.")
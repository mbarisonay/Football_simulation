import pandas as pd
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
import time
import random
import os
import subprocess

# --- AYARLAR ---
# Ana dosyanÄ±n yolu (URL'leri buradan alacaÄŸÄ±z)
SOURCE_FILE = "ALL_LEAGUES_DETAILED_MATCHES.csv"
# KartlarÄ±n kaydedileceÄŸi yeni dosya
OUTPUT_FILE = "MATCH_CARDS.csv"


# --- DRIVER YÃ–NETÄ°MÄ° ---
def init_driver():
    options = uc.ChromeOptions()
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--no-sandbox")
    options.page_load_strategy = 'eager'  # HÄ±z iÃ§in
    try:
        d = uc.Chrome(options=options, use_subprocess=True)
        d.set_page_load_timeout(60)
        return d
    except:
        time.sleep(5)
        return init_driver()


# --- DOSYA OKUMA ---
if not os.path.exists(SOURCE_FILE):
    print(f"âŒ HATA: {SOURCE_FILE} bulunamadÄ±. Ã–nce diÄŸer scraper'Ä± Ã§alÄ±ÅŸtÄ±rÄ±n.")
    exit()

# Sadece URL'leri alÄ±yoruz
df_source = pd.read_csv(SOURCE_FILE, usecols=['MatchURL'])
all_urls = df_source['MatchURL'].unique().tolist()

# Daha Ã¶nce Ã§ekilen kartlar varsa onlarÄ± atlayalÄ±m
scraped_urls = set()
if os.path.exists(OUTPUT_FILE):
    try:
        df_existing = pd.read_csv(OUTPUT_FILE)
        scraped_urls = set(df_existing['MatchURL'].tolist())
        print(f"ğŸ“¥ {len(scraped_urls)} maÃ§Ä±n kart verisi zaten var, atlanacak.")
    except:
        pass

# Ã‡ekilecek URL listesi
urls_to_scrape = [u for u in all_urls if u not in scraped_urls]
print(f"ğŸš€ Toplam {len(urls_to_scrape)} maÃ§Ä±n kart verisi Ã§ekilecek...")

driver = init_driver()


# --- PARSER ---
def extract_cards_only(soup):
    """ Sadece kart ikonlarÄ±nÄ± sayar """
    # "Cards" baÅŸlÄ±ÄŸÄ±nÄ± bul
    header = soup.find(lambda tag: tag.name in ["div", "th"] and "Cards" in tag.get_text())
    hy, hr, ay, ar = 0, 0, 0, 0

    if header:
        container = None
        if header.name == "th":
            container = header.find_parent("tr").find_next_sibling("tr")
        else:
            # Div yapÄ±sÄ±
            container = header.find_parent("tr")
            if container: container = container.find_next_sibling("tr")

        if container:
            cols = container.find_all("td")  # veya div
            if not cols: cols = container.find_all("div", recursive=False)

            if len(cols) >= 2:
                # Ev Sahibi
                hy = len(cols[0].select('.yellow_card'))
                hr = len(cols[0].select('.red_card')) + len(cols[0].select('.yellow_red_card'))

                # Deplasman
                ay = len(cols[1].select('.yellow_card'))
                ar = len(cols[1].select('.red_card')) + len(cols[1].select('.yellow_red_card'))

    return hy, hr, ay, ar


# --- DÃ–NGÃœ ---
try:
    for i, url in enumerate(urls_to_scrape, 1):
        try:
            driver.get(url)
            time.sleep(random.uniform(1.5, 3.0))  # Sadece kart bakacaÄŸÄ±mÄ±z iÃ§in biraz hÄ±zlÄ± olabilir

            soup = BeautifulSoup(driver.page_source, "html.parser")

            # KartlarÄ± Ã‡ek
            hy, hr, ay, ar = extract_cards_only(soup)

            card_data = {
                "MatchURL": url,
                "HomeYellowCards": hy,
                "HomeRedCards": hr,
                "AwayYellowCards": ay,
                "AwayRedCards": ar
            }

            # AnlÄ±k KayÄ±t
            df_single = pd.DataFrame([card_data])
            header_mode = not os.path.exists(OUTPUT_FILE)
            df_single.to_csv(OUTPUT_FILE, mode='a', header=header_mode, index=False)

            print(f"âœ… {i}/{len(urls_to_scrape)} Kartlar: ğŸŸ¨ {hy}-{ay} | ğŸŸ¥ {hr}-{ar}")

        except Exception as e:
            print(f"âš ï¸ Hata ({url}): {e}")
            # Hata olsa bile driver'Ä± yeniden baÅŸlatÄ±p devam etmeye Ã§alÄ±ÅŸ
            try:
                driver.quit(); driver = init_driver()
            except:
                pass

except KeyboardInterrupt:
    print("\nğŸ›‘ Durduruldu.")

finally:
    if driver: driver.quit()
    print("\nğŸ Kart Ã§ekme iÅŸlemi bitti.")
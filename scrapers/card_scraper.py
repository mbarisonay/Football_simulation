import pandas as pd
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
import time
import random
import os
import sys
import subprocess

# --- YOL AYARLARI ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
BASE_DIR = os.path.dirname(CURRENT_DIR)

# Hedef Dosya: Ä°ÅŸlenmiÅŸ Ana Veri Seti
SOURCE_FILE = os.path.join(BASE_DIR, "data", "processed", "MASTER_MATCH_STATS.csv")

# KartlarÄ±n KaydedileceÄŸi Yer (Raw klasÃ¶rÃ¼ne atalÄ±m, sonra birleÅŸtiririz)
OUTPUT_DIR = os.path.join(BASE_DIR, "data", "raw")
if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "MATCH_CARDS.csv")

# --- KONTROLLER ---
if not os.path.exists(SOURCE_FILE):
    print(f"âŒ HATA: '{SOURCE_FILE}' bulunamadÄ±!")
    print("   Ã–nce 'etl/merge_data.py' (final_merge) dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±p Master dosyayÄ± oluÅŸturun.")
    sys.exit()

print(f"ğŸ“‚ Kaynak Dosya: {SOURCE_FILE}")


# --- DRIVER YÃ–NETÄ°MÄ° ---
def kill_chrome():
    try:
        subprocess.call("taskkill /F /IM chrome.exe /T", shell=True, stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL)
        subprocess.call("taskkill /F /IM chromedriver.exe /T", shell=True, stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL)
    except:
        pass


def init_driver():
    kill_chrome()
    options = uc.ChromeOptions()
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--no-sandbox")
    options.page_load_strategy = 'eager'
    try:
        d = uc.Chrome(options=options, use_subprocess=True)
        d.set_page_load_timeout(60)
        return d
    except:
        time.sleep(5)
        return init_driver()


# --- URL LÄ°STESÄ°NÄ° HAZIRLA ---
try:
    df_source = pd.read_csv(SOURCE_FILE)

    # MatchURL sÃ¼tunu var mÄ± kontrol et
    if 'MatchURL' not in df_source.columns:
        print("âŒ HATA: Master dosyasÄ±nda 'MatchURL' sÃ¼tunu yok!")
        print("   Link olmadan kartlarÄ± Ã§ekemeyiz. Merge iÅŸleminde URL'lerin silinmediÄŸinden emin olun.")
        sys.exit()

    all_urls = df_source['MatchURL'].dropna().unique().tolist()
except Exception as e:
    print(f"âŒ Dosya okuma hatasÄ±: {e}")
    sys.exit()

# Zaten Ã§ekilmiÅŸ kartlar varsa atla
scraped_urls = set()
if os.path.exists(OUTPUT_FILE):
    try:
        df_existing = pd.read_csv(OUTPUT_FILE)
        scraped_urls = set(df_existing['MatchURL'].tolist())
        print(f"ğŸ“¥ {len(scraped_urls)} maÃ§Ä±n kart verisi zaten Ã§ekilmiÅŸ, atlanacak.")
    except:
        pass

urls_to_scrape = [u for u in all_urls if u not in scraped_urls]
print(f"ğŸš€ Toplam {len(urls_to_scrape)} maÃ§ taranacak...")

if not urls_to_scrape:
    print("âœ… TÃ¼m maÃ§larÄ±n kart verisi zaten var. Ä°ÅŸlem yapmaya gerek yok.")
    sys.exit()

driver = init_driver()


# --- PARSER ---
def extract_cards_only(soup):
    """ Kart Ä°konlarÄ±nÄ± Sayar """
    header = soup.find(lambda tag: tag.name in ["div", "th"] and "Cards" in tag.get_text())
    hy, hr, ay, ar = 0, 0, 0, 0

    if header:
        container = None
        if header.name == "th":
            container = header.find_parent("tr").find_next_sibling("tr")
        else:
            # Div yapÄ±sÄ± iÃ§in en yakÄ±n tablo satÄ±rÄ±nÄ± bul
            container = header.find_parent("tr")
            if container: container = container.find_next_sibling("tr")

        if container:
            # .cards class'Ä±na sahip divleri bul
            # BazÄ± sayfalarda td iÃ§inde, bazÄ±larÄ±nda div iÃ§inde olabilir
            card_divs = container.select(".cards")

            if len(card_divs) >= 2:
                # Ev Sahibi
                hy = len(card_divs[0].select('.yellow_card'))
                hr = len(card_divs[0].select('.red_card')) + len(card_divs[0].select('.yellow_red_card'))
                # Deplasman
                ay = len(card_divs[1].select('.yellow_card'))
                ar = len(card_divs[1].select('.red_card')) + len(card_divs[1].select('.yellow_red_card'))

    return hy, hr, ay, ar


# --- DÃ–NGÃœ ---
try:
    for i, url in enumerate(urls_to_scrape, 1):
        try:
            driver.get(url)
            # Kartlar hÄ±zlÄ± yÃ¼klenir
            time.sleep(random.uniform(1.0, 2.5))

            soup = BeautifulSoup(driver.page_source, "html.parser")

            hy, hr, ay, ar = extract_cards_only(soup)

            card_data = {
                "MatchURL": url,
                "HomeYellowCards": hy,
                "HomeRedCards": hr,
                "AwayYellowCards": ay,
                "AwayRedCards": ar
            }

            df_single = pd.DataFrame([card_data])
            header_mode = not os.path.exists(OUTPUT_FILE)
            df_single.to_csv(OUTPUT_FILE, mode='a', header=header_mode, index=False)

            print(f"âœ… {i}/{len(urls_to_scrape)} | ğŸŸ¨ {hy}-{ay} ğŸŸ¥ {hr}-{ar} | {url.split('/')[-1]}")

        except Exception as e:
            print(f"âš ï¸ Hata: {e}")
            try:
                driver.quit(); driver = init_driver()
            except:
                pass

except KeyboardInterrupt:
    print("\nğŸ›‘ Durduruldu.")

finally:
    if driver: driver.quit()
    print("\nğŸ Ä°ÅŸlem bitti.")